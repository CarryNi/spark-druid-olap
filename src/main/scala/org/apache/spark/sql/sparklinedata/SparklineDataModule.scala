/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.sparklinedata

import org.apache.spark.sql.{SPLLogging, SQLContext, SparkSession, Strategy}
import org.apache.spark.sql.catalyst.ScalaReflection
import org.apache.spark.sql.catalyst.optimizer.Optimizer
import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan
import org.apache.spark.sql.catalyst.rules.{Rule, RuleExecutor}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.hive.sparklinedata.{SparklineDataParser, SparklineDruidCommandsParser}
import org.apache.spark.sql.internal.SQLConf.SQLConfigBuilder
import org.apache.spark.sql.planner.logical.DruidLogicalOptimizer
import org.apache.spark.sql.sources.druid.{DruidPlanner, DruidStrategy}
import org.sparklinedata.spark.dateTime.Functions

trait SparklineDataModule {

  /**
    * Function registrations.
    *
    * @param sparkSession
    */
  def registerFunctions(sparkSession: SparkSession) : Unit = {}

  /**
    * Any extra rules that should be added to the Logical Optimizer.
    * @return
    */
  def logicalRules : Seq[(String, Rule[LogicalPlan])] = Nil

  /**
    * An optinal parser extension
    *
    * @param sparkSession
    * @return
    */
  def parser(sparkSession: SparkSession) : Option[SparklineDataParser] = None

  /**
    * An optional transform of LogicalPlan generated by the parser.
    * These apply changes before any semantic analysis has happened.
    * Use it for very coarse transformations like:
    * - removing invalid sort expressions.
    *
    * @param sparkSession
    * @return
    */
  def parsedTransform(sparkSession: SparkSession) : Option[RuleExecutor[LogicalPlan]] = None

  def physicalRules(sparkSession: SparkSession) : Seq[Strategy] = Nil

}

object BaseModule extends SparklineDataModule {

  override def registerFunctions(sparkSession: SparkSession) = {
    Functions.register(sparkSession.sqlContext)
  }

  override def logicalRules : Seq[(String, Rule[LogicalPlan])] =
    DruidLogicalOptimizer.batches

  override def parser(sparkSession: SparkSession) : Option[SparklineDataParser] =
    Some(new SparklineDruidCommandsParser(sparkSession))

  override def physicalRules(sparkSession: SparkSession) : Seq[Strategy] = {
    val dP = DruidPlanner(sparkSession.sqlContext)
    Seq(new DruidStrategy(dP))
  }

}

class ModuleLoader(sparkSession: SparkSession,
                   val modules : Seq[SparklineDataModule]) {

  def registerFunctions : Unit = {
    modules.foreach { m =>
      m.registerFunctions(sparkSession)
    }
  }

  def addLogicalRules : Unit = {
    val batches = modules.flatMap(_.logicalRules).map(_._2)
    sparkSession.experimental.extraOptimizations =
      batches ++ sparkSession.experimental.extraOptimizations
  }

  def addPhysicalRules : Unit = {
    val pRules = modules.flatMap(_.physicalRules(sparkSession))
    sparkSession.experimental.extraStrategies =
      pRules ++ sparkSession.experimental.extraStrategies
  }

  def parsers : Seq[SparklineDataParser] = {
    modules.flatMap(_.parser(sparkSession).toSeq)
  }

  def parserTransformers : Seq[RuleExecutor[LogicalPlan]] = {
    modules.flatMap(_.parsedTransform(sparkSession).toSeq)
  }

}

object ModuleLoader extends SPLLogging {

  private var modules : Seq[SparklineDataModule] = _

  private def loadModules(sparkSession : SparkSession) : Unit = synchronized {

    if (modules == null ) {

      val runtimeMirror = ScalaReflection.mirror
      val modulesToLoad = sparkSession.conf.get(ModuleLoader.SPARKLINE_MODULES)

      modules = Seq(BaseModule) ++
        modulesToLoad.map { m =>

          val module = runtimeMirror.staticModule(m)
          val obj = runtimeMirror.reflectModule(module)
          val o = obj.instance.asInstanceOf[SparklineDataModule]
          log.info(s"loaded sparklinedata module '$m'")
          o
        }
    }
  }

  val SPARKLINE_MODULES = SQLConfigBuilder(
    "spark.sparklinedata.modules").
    doc("sparkline modules to load.").
    stringConf.toSequence.createWithDefault(Seq())

  def apply(sparkSession : SparkSession) : ModuleLoader = {
    loadModules(sparkSession)
    new ModuleLoader(sparkSession, modules)
  }
}